# Soonyeul Park <ardentpark@gmail.com>, 2018. #zanata
# Sungjin Kang <gang.sungjin@gmail.com>, 2018. #zanata
# Jongwon Lee <tothebinaryworld@gmail.com>, 2019. #zanata
# Sungjin Kang <gang.sungjin@gmail.com>, 2019. #zanata
# SeongHo Park <glgl2131@naver.com>, 2020. #zanata
msgid ""
msgstr ""
"Project-Id-Version: openstack-helm 0.1.1.dev3114\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2020-07-27 18:12+0000\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"PO-Revision-Date: 2020-07-28 12:33+0000\n"
"Last-Translator: SeongHo Park <glgl2131@naver.com>\n"
"Language-Team: Korean (South Korea)\n"
"Language: ko_KR\n"
"X-Generator: Zanata 4.3.3\n"
"Plural-Forms: nplurals=1; plural=0\n"

msgid "**Check Chart Status**"
msgstr "**Chart 상태 확인**"

msgid ""
"**First**, edit the ``values.yaml`` for Neutron, Glance, Horizon, Keystone, "
"and Nova."
msgstr ""
"**첫째**, Neutron, Glance, Horizon, Keystone, and Nova 를 위해 ``values."
"yaml`` 를 편집하시오."

msgid "**Review the Ingress configuration.**"
msgstr "**Ingress 구성을 검토합니다.**"

msgid ""
"**Second** option would be as ``--set`` flags when calling ``helm install``"
msgstr ""
"``helm install`` 을 호출할 때 **두 번째** 옵션은 ``--set``가 될 것입니다."

msgid ""
"**TaaS Agent, TaaS OVS Driver and TaaS SR-IOV Driver**: This forms the back-"
"end of TaaS which runs as a ML2 agent extension on compute nodes. It handles "
"the RPC calls made by TaaS Plugin and configures the mechanism driver, i.e. "
"OpenVSwitch or SR-IOV Nic Switch."
msgstr ""
"**TaaS 에이전트, TaaS OVS 드라이버, TaaS SR-IOV 드라이버**: Compute 노드에서 "
"ML2 에이전트 확장으로 실행되는 TaaS 백엔드 서버입니다. TaaS 플러그인이 만든 "
"RPC 콜을 처리하고 OpenVSwitch나 SR-IOV Nic 스위치와 같은 메커니즘 드라이버를 "
"구성합니다."

msgid ""
"**TaaS Dashboard Plugin**: Horizon Plugin which adds GUI panels for TaaS "
"resources in the Horizon Dashboard."
msgstr ""
"**TaaS Dashboard 플러그인**: Horizon Dashboard에서 TaaS 리소스를 위한 GUI 패"
"널을 추가하는 Horizon 플러그인입니다."

msgid ""
"**TaaS Plugin**: This is the front-end of TaaS which runs on controller node "
"(Neutron server). This serves TaaS APIs and stores/retrieves TaaS "
"configuration state to/from Neutron TaaS DB."
msgstr ""
"**TaaS Plugin**: 컨트놀러 노드 (Neutron 서버)에서 실행되는 TaaS 프론트 서버입"
"니다. TaaS API를 제공하고 Neutron TaaS DB에서 TaaS 구성 상태를 저장, 검색합니"
"다."

msgid "16GB of RAM"
msgstr "16GB of RAM"

msgid "4 Cores"
msgstr "4 Cores"

msgid "48GB HDD"
msgstr "48GB HDD"

msgid "8 Cores"
msgstr "8 Cores"

msgid "8GB of RAM"
msgstr "8GB of RAM"

msgid "Activate the OpenStack namespace to be able to use Ceph"
msgstr "Ceph를 사용하려면 OpenStack 네임 스페이스를 활성화하십시오"

msgid "Activate the openstack namespace to be able to use Ceph"
msgstr "Ceph를 사용할 수 있도록 OpenStack namespace를 활성화합니다."

msgid ""
"Add the address of the Kubernetes API, ``172.17.0.1``, and ``.svc.cluster."
"local`` to your ``no_proxy`` and ``NO_PROXY`` environment variables."
msgstr ""
"Kubernetes API의 주소 ``172.17.0.1`` 과 ``.svc.cluster.local`` 을 "
"``no_proxy`` 와 ``NO_PROXY`` 환경 변수에 추가하십시오."

msgid ""
"Add to the Install steps these flags - also adding a shell environment "
"variable to save on repeat code."
msgstr ""
"플래그들을 설치 과정에 추가하십시오 - 또한 반복 코드를 저장할 셸 환경 변수를 "
"추가하십시오."

msgid ""
"Additional configuration variables can be found `here <https://github.com/"
"openstack/openstack-helm-infra/blob/master/roles/deploy-kubeadm-aio-common/"
"defaults/main.yml>`_. In particular, ``kubernetes_cluster_pod_subnet`` can "
"be used to override the pod subnet set up by Calico (the default container "
"SDN), if you have a preexisting network that conflicts with the default pod "
"subnet of 192.168.0.0/16."
msgstr ""
"추가 구성 변수는 `여기 <https://github.com/openstack/openstack-helm-infra/"
"blob/master/roles/deploy-kubeadm-aio-common/defaults/main.yml>`_ 에서 확인할 "
"수 있습니다. 특히 ``kubernetes_cluster_pod_subnet`` 은 기본 pod 서브넷 "
"192.168.0.0/16 과 충돌하는 기존 네트워크가 있는 경우, Calico (기본 컨테이너 "
"SDN) 이 설정한 pod 서브넷을 무시하여 사용할 수 있습니다."

msgid ""
"Additional information on Kubernetes Ceph-based integration can be found in "
"the documentation for the `CephFS <https://github.com/kubernetes-incubator/"
"external-storage/blob/master/ceph/cephfs/README.md>`_ and `RBD <https://"
"github.com/kubernetes-incubator/external-storage/blob/master/ceph/rbd/README."
"md>`_ storage provisioners, as well as for the alternative `NFS <https://"
"github.com/kubernetes-incubator/external-storage/blob/master/nfs/README."
"md>`_ provisioner."
msgstr ""
"Kubernetes을 Ceph 기반으로 통합하기 위한 추가 정보는 `CephFS  <https://"
"github.com/kubernetes-incubator/external-storage/blob/master/ceph/cephfs/"
"README.md>`_ 와 `RBD <https://github.com/kubernetes-incubator/external-"
"storage/blob/master/ceph/rbd/README.md>`_ 스토리지 공급자와 대체 `NFS "
"<https://github.com/kubernetes-incubator/external-storage/blob/master/nfs/"
"README.md>`_ 공급자에 대한 문서에서 찾을 수 있습니다."

msgid ""
"After making the configuration changes, run a ``make`` and then install as "
"you would from AIO or MultiNode instructions."
msgstr ""
"구성을 변경한 후, ``make``를 실행하고 AIO 또는 다중 노드 명령에서 했던 바와 "
"같이 설치하십시오."

msgid ""
"All commands below should be run as a normal user, not as root. Appropriate "
"versions of Docker, Kubernetes, and Helm will be installed by the playbooks "
"used below, so there's no need to install them ahead of time."
msgstr ""
"아래의 모든 명령은 root가 아닌 일반 사용자로 실행해야 합니다. Docker, "
"Kubernetes, 그리고 Helm의 적절한 버전이 아래에 사용된 playbook에 의해 설치될 "
"것이므로, 사전에 설치할 필요가 없습니다."

msgid ""
"Alternatively, this step can be performed by running the script directly:"
msgstr "대안으로, 직접 스크립트를 실행하여 이 단계를 수행할 수 있습니다:"

msgid ""
"An Ingress is a collection of rules that allow inbound connections to reach "
"the cluster services."
msgstr ""
"Ingress는 인바운드 연결이 클러스터 서비스에 도달하도록 허락하는 규칙들의 모음"
"입니다."

msgid ""
"As any other Neutron plugin, TaaS neutron plugin functionality consists of "
"following modules:"
msgstr ""
"다른 Neutron 플러그인과 마찬가지로 TaaS Neutron 플러그인 기능은 다음 모듈로 "
"구성됩니다."

msgid ""
"Before deploying TaaS and/or TaaS Dashboard, it needs to be added in Neutron "
"and/or Horizon LOCI images."
msgstr ""
"TaaS, TaaS Dashboard를 배포하기 전, Neutron, Horizon LOCI 이미지를 추가해야 "
"합니다."

msgid ""
"By default the Calico CNI will use ``192.168.0.0/16`` and Kubernetes "
"services will use ``10.96.0.0/16`` as the CIDR for services. Check that "
"these CIDRs are not in use on the development node before proceeding, or "
"adjust as required."
msgstr ""
"기본적으로 Calico CNI는 ``192.168.0.0/16`` 을 사용할 것이고 Kubernetes 서비스"
"는 ``10.96.0.0/16`` 을 서비스를 위한 CIDR로 사용할 것입니다. 진행하기 전에 "
"이 CIDR이 개발 노드에서 사용하지 않는지 확인하거나 필요에 따라 조정하십시오."

msgid ""
"By default, this installation will use Google DNS Server IPs (8.8.8.8, "
"8.8.4.4) and will update resolv.conf as a result. If those IPs are blocked "
"by the proxy, this will overwrite the original DNS entries and result in the "
"inability to connect to anything on the network behind the proxy. These DNS "
"nameserver entries can be changed by updating the "
"``external_dns_nameservers`` entry in this file:"
msgstr ""
"기본적으로, 이 설치는 구글 DNS 서버 IP(8.8.8.8, 8.8.4.4)를 사용하며 결과적으"
"로 resolv.conf를 업데이트 합니다. 해당 IP가 프록시에 의해 차단된 경우 원래 "
"DNS 항목을 덮어 쓰고 프록시 뒤에 있는 어떤 항목에도 연결할 수 없습니다. 이 "
"DNS 네임서버 항목은 이 파일에서 ``external_dns_nameservers`` 항목을 업데이트"
"하여 변경할 수 있습니다."

msgid ""
"Cinder deployment is not tested in the OSH development environment community "
"gates"
msgstr "Cinder 배포는 OSH 개발 환경 커뮤니티 게이트에서 테스트되지 않았습니다."

msgid "Cleaning the Deployment"
msgstr "배포 정리"

msgid "Clone the OpenStack-Helm Repos"
msgstr "OpenStack-Helm 저장소 Clone"

msgid "Code examples below."
msgstr "아래의 코드 예제를 확인하세요."

msgid "Common Deployment Requirements"
msgstr "일반적인 배포 요구 사항"

msgid "Configure OpenStack"
msgstr "OpenStack 구성"

msgid ""
"Configuring OpenStack for a particular production use-case is beyond the "
"scope of this guide. Please refer to the OpenStack `Configuration <https://"
"docs.openstack.org/latest/configuration/>`_ documentation for your selected "
"version of OpenStack to determine what additional values overrides should be "
"provided to the OpenStack-Helm charts to ensure appropriate networking, "
"security, etc. is in place."
msgstr ""
"특정 프로덕션 유즈 케이스에 맞게 OpenStack을 구성하는 방법은 이 가이드에서 지"
"원하지 않습니다. 선택한 OpenStack 버전에 대한 OpenStack `구성 <https://docs."
"openstack.org/latest/configuration/>`_ 문서를 참조하여 적절한 네트워킹, 보안 "
"등을 위한 OpenStack-Helm chart에 어떤 추가 값이 작성되어야 하는지 결정하십시"
"오."

msgid "Contents:"
msgstr "목차:"

msgid ""
"Copy the key: ``sudo cp ~/.ssh/id_rsa /etc/openstack-helm/deploy-key.pem``"
msgstr "키 복사: ``sudo cp ~/.ssh/id_rsa/openstack-helm/deploy-key.pem``"

msgid "Create a patchset for ``openstack/requirements`` repo"
msgstr "``openstack/requirements`` 레포에서 패치셋 생성"

msgid "Create an environment file"
msgstr "환경 파일 생성"

msgid "Create an inventory file"
msgstr "인벤토리 파일 생성"

msgid ""
"Create an ssh-key on the master node, and add the public key to each node "
"that you intend to join the cluster."
msgstr ""
"Master 노드에 ssh-key를 생성합니다. 그리고 클러스터로 묶을 각 노드에 공개 키"
"를 추가합니다."

msgid "Deploy Barbican"
msgstr "Barbican 배포"

msgid "Deploy Ceph"
msgstr "Ceph 배포"

msgid "Deploy Cinder"
msgstr "Cinder 배포"

msgid "Deploy Compute Kit (Nova and Neutron)"
msgstr "컴퓨팅 키트 (Nova and Neutron) 배포"

msgid "Deploy Glance"
msgstr "Glance 배포"

msgid "Deploy Heat"
msgstr "Heat 배포"

msgid "Deploy Horizon"
msgstr "Horizon 배포"

msgid "Deploy Keystone"
msgstr "Keystone 배포"

msgid "Deploy Kubernetes & Helm"
msgstr "Kubernetes & Helm 배포"

msgid "Deploy Libvirt"
msgstr "Libvirt 배포"

msgid "Deploy MariaDB"
msgstr "MariaDB 배포"

msgid "Deploy Memcached"
msgstr "Memcached 배포"

msgid "Deploy NFS Provisioner"
msgstr "NFS Provisioner 배포"

msgid "Deploy OVS-DPDK"
msgstr "OVS-DPDK 배포"

msgid "Deploy OpenStack-Helm"
msgstr "OpenStack-Helm 배포"

msgid "Deploy OpenvSwitch"
msgstr "OpenvSwitch 배포"

msgid "Deploy RabbitMQ"
msgstr "RabbitMQ 배포"

msgid "Deploy Rados Gateway for object store"
msgstr "개체 저장을 위한 Rados Gateway 배포"

msgid "Deploy tap-as-a-service (TaaS) Neutron / Dashboard plugin"
msgstr "TaaS(Tap-as-a-Service) Neutron / Dashboard 플러그인 배포"

msgid "Deploy the ingress controller"
msgstr "ingress controller 배포"

msgid "Deployment With Ceph"
msgstr "Ceph를 사용한 배포"

msgid "Deployment With NFS"
msgstr "NFS를 사용한 배포"

msgid "Development"
msgstr "개발"

msgid "Environment tear-down"
msgstr "환경 해제"

msgid ""
"Essentially the use of Ingress for OpenStack-Helm is an Nginx proxy service. "
"Ingress (Nginx) is accessible by your cluster public IP - e.g. the IP "
"associated with ``kubectl get pods -o wide --all-namespaces | grep ingress-"
"api`` Ingress/Nginx will be listening for server name requests of \"keystone"
"\" or \"keystone.openstack\" and will route those requests to the proper "
"internal K8s Services. These public listeners in Ingress must match the "
"external DNS that you will set up to access your OpenStack deployment. Note "
"each rule also has a Service that directs Ingress Controllers allow access "
"to the endpoints from within the cluster."
msgstr ""
"본질적으로 OpenStack-Helm을 위한 Ingress의 사용은 Nginx 프록시 서비스입니다. "
"Ingress(Nginx)는 클러스터 공용 IP를 통해 접근할 수 있습니다. - 예시, "
"``kubectl get pods -o wide --all-namespace | grep ingress-api`` 와 관련된 "
"IP. Ingress/Nginx 는 \\\"keystone\\\" 또는 \\\"keystone.openstack\\\" 의 서버"
"명 요청을 수신 대기하고 요청들을 적절한 내부 K8s 서비스로 경로를 설정할 것입"
"니다. Ingress의 공용 리스너는 OpenStack 배포에 액세스하도록 설정할 외부 DNS"
"와 일치해야 합니다. 각 규칙에는 Ingress Controller가 클러스터 내에서 endpoint"
"에 대한 접근을 허용하도록 지시하는 서비스 또한 있음을 참고하십시오."

msgid "Examples"
msgstr "예제"

msgid "Exercise the Cloud"
msgstr "클라우드 실습"

msgid "External DNS and FQDN"
msgstr "외부 DNS와 FQDN"

msgid "External DNS to FQDN/Ingress"
msgstr "외부 DNS에서 FQDN/Ingress"

msgid ""
"For ``identity`` and ``dashboard`` at ``host_fdqn_override.public`` replace "
"``null`` with the value as ``keystone.os.foo.org`` and ``horizon.os.foo.org``"
msgstr ""
"``host_fdqn_override.public`` 의 ``identity`` 와 ``dashboard`` 에서 ``null`` "
"값을 ``keystone.os.foo.org`` 와 ``horizon.os.foo.org`` 으로 바꾸십시오."

msgid ""
"For a deployment without cinder and horizon the system requirements are:"
msgstr "cinder와 horizon 없이 배포하기 위한 시스템 요구 사항:"

msgid ""
"For a lab or proof-of-concept environment, the OpenStack-Helm gate scripts "
"can be used to quickly deploy a multinode Kubernetes cluster using KubeADM "
"and Ansible. Please refer to the deployment guide `here <./kubernetes-gate."
"html>`__."
msgstr ""
"Lab이나 PoC 환경인 경우, OpenStack-Helm 게이트 스크립트를 이용하여 KubeADM과 "
"Ansible을 사용한 멀티노드 Kubernetes 클러스터를 빠르게 구성할 수 있습니다. 자"
"세한 배포 가이드는 `여기 <./kubernetes-gate.html>`__ 를 참조하십시오."

msgid "For more details, refer to TaaS specification: Tap-as-a-service_."
msgstr "더 자세한 내용은 TaaS 사양서를 참조하십시오: Tap-as-a-service_"

msgid ""
"For other deployment options, select appropriate ``Deployment with ...`` "
"option from `Index <../developer/index.html>`__ page."
msgstr ""
"다른 배포 옵션의 경우, `Index <../developer/index.html>`__ 페이지에서 적절한 "
"``Deployment with ...`` 옵션을 선택하십시오."

msgid "Gate-Based Kubernetes"
msgstr "Gate 기반 Kubernetes"

msgid "Get the Nginx configuration from the Ingress Pod:"
msgstr "Ingress Pod에서 Ningx 구성을 가져옵니다:"

msgid "Get the ``helm status`` of your chart."
msgstr "Chart에서 ``helm status`` 를 가져옵니다."

msgid "Helm Chart Installation"
msgstr "Helm Chart 설치"

msgid ""
"Horizon deployment is not tested in the OSH development environment "
"community gates"
msgstr ""
"Horizon 배포는 OSH 개발 환경 커뮤니티 게이트에서 테스트되지 않았습니다."

msgid "Host Configuration"
msgstr "호스트 구성"

msgid ""
"If doing an `AIO install <https://docs.openstack.org/openstack-helm/latest/"
"install/developer/index.html>`__, all the ``--set`` flags"
msgstr ""
"만약 `AIO install <https://docs.openstack.org/openstack-helm/latest/install/"
"developer/index.html>`__ 을 하려면, 모두 ``--set`` 플러그를 설정합니다."

msgid ""
"Implementing the FQDN overrides **must** be done at install time. If you run "
"these as helm upgrades, Ingress will notice the updates though none of the "
"endpoint build-out jobs will run again, unless they are cleaned up manually "
"or using a tool like Armada."
msgstr ""
"설치 시에 FQDN 오버라이드 구현을 **반드시** 해야합니다. helm 업그레이드로 실"
"행하면, 수동으로 정리되거나 Armada와 같은 도구를 사용하지 않는 한, 어떤 "
"endpoint 구축 작업도 재실행되지 않을 것이기 때문에  Ingress는 업데이트를 알"
"릴 것입니다. "

msgid ""
"In order to access your OpenStack deployment on Kubernetes we can use the "
"Ingress Controller or NodePorts to provide a pathway in. A background on "
"Ingress, OpenStack-Helm fully qualified domain name (FQDN) overrides, "
"installation, examples, and troubleshooting will be discussed here."
msgstr ""
"Kubernetes에서 OpenStack 배포에 접근하기 위해 Ingress Controller 또는 진입 경"
"로를 제공하는 NodePorts 를 사용할 수 있습니다. Ingress의 배경지식으로, "
"OpenStack-Helm의 fully qualified domain name(FQDN) 오버라이드, 설치, 예시, 그"
"리고 트러블슈팅이 여기서 논의될 것입니다."

msgid ""
"In order to deploy OpenStack-Helm behind corporate proxy servers, add the "
"following entries to ``openstack-helm-infra/tools/gate/devel/local-vars."
"yaml``."
msgstr ""
"기업 프록시 서버에서 OpenStack-Helm을 배포하려면,  ``openstack-helm-infra/"
"tools/gate/devel/local-vars.yaml`` 에 다음의 항목들을 추가하십시오."

msgid ""
"In order to drive towards a production-ready OpenStack solution, our goal is "
"to provide containerized, yet stable `persistent volumes <https://kubernetes."
"io/docs/concepts/storage/persistent-volumes/>`_ that Kubernetes can use to "
"schedule applications that require state, such as MariaDB (Galera). Although "
"we assume that the project should provide a \"batteries included\" approach "
"towards persistent storage, we want to allow operators to define their own "
"solution as well. Examples of this work will be documented in another "
"section, however evidence of this is found throughout the project. If you "
"find any issues or gaps, please create a `story <https://storyboard."
"openstack.org/#!/project/886>`_ to track what can be done to improve our "
"documentation."
msgstr ""
"제품으로 사용이 완료된 OpenStack 솔루션을 목표로 우리는 Kubernetes 가 "
"MariaDB (Galera)와 같이 상태를 필요로하는 스케쥴 애플리케이션에서 사용할 수 "
"있는 컨테이너화된 안정적인 `persistent volumes <https://kubernetes.io/docs/"
"concepts/storage/persistent-volumes/>`_ 를 제공하는 것입니다. 프로젝트에서 "
"presistent storage에 대한 \"배터리 포함(batteries included)\" 접근 방식을 제"
"공해야한다고 가정하지만 운영자가 자체 솔루션을 정의할 수 있도록 하고 싶습니"
"다. 이 작업에 대한 예제는 다른 절에서 문서화 될 것이지만, 이 증명은 프로젝트 "
"전체에서 확인됩니다. 문제가 있거나 틈이 발견되면 문서를 개선하기 위해 수행할 "
"수 있는 `story <https://storyboard.openstack.org/#!/project/886>`_ 를 생성하"
"여 추적할 수 있도록 작성하십시오."

msgid "Ingress"
msgstr "Ingress"

msgid "Install OpenStack-Helm"
msgstr "OpenStack-Helm 설치"

msgid "Installation"
msgstr "설치"

msgid ""
"It can be configured to give services externally-reachable URLs, load "
"balance traffic, terminate SSL, offer name based virtual hosting, and more."
msgstr ""
"외부적으로 도달 가능한 URL, 로드 밸런스 트래픽, SSL 종료, 이름 기반 가상 호스"
"팅 제공 등의 서비스를 제공하도록 구성할 수 있습니다."

msgid "Kubernetes Preparation"
msgstr "Kubernetes 준비"

msgid "Kubernetes and Common Setup"
msgstr "Kubernetes와 일반 설치"

msgid "Latest Version Installs"
msgstr "최신 버전 설치"

msgid ""
"Look for *server* configuration with a *server_name* matching your desired "
"FQDN"
msgstr ""
"원하는 FQDN과 일치하는 *server_name* 을 가진 *server* 에대한 구성을 찾습니다"

msgid ""
"Managing and configuring a Kubernetes cluster is beyond the scope of "
"OpenStack-Helm and this guide."
msgstr ""
"Kubernetes 클러스터를 관리하고 구성하는 내용은 OpenStack-Helm과 이 가이드에서"
"는 다루지 않습니다."

msgid ""
"Many of the default container images that are referenced across OpenStack-"
"Helm charts are not intended for production use; for example, while LOCI and "
"Kolla can be used to produce production-grade images, their public reference "
"images are not prod-grade.  In addition, some of the default images use "
"``latest`` or ``master`` tags, which are moving targets and can lead to "
"unpredictable behavior.  For production-like deployments, we recommend "
"building custom images, or at minimum caching a set of known images, and "
"incorporating them into OpenStack-Helm via values overrides."
msgstr ""
"OpenStack-Helm chart 에서 참조되는 많은 기본 컨테이너 이미지는 실사용의 용도"
"로 사용되지 않습니다. 예를 들어 LOCI와 Kolla는 프로덕션 등급 이미지를 구성하"
"는데 사용할 수 있지만 공개 참조 이미지는 생산성이 떨어집니다. 또한 일부 기본 "
"이미지는 ``latest`` 또는 ``master`` 태그를 사용하는데, 이는 변경될 수 있는 태"
"그이기에 예측 할 수 없는 행동을 유발할 수 있습니다. 프로덕션과 같은 배포의 경"
"우, 커스텀 이미지를 작성하거나 알려진 이미지 세트를 캐싱하고 값을 재정의하여 "
"OpenStack-Helm에 통합하는 것이 좋습니다."

msgid "Multinode"
msgstr "멀티노드"

msgid "Neutron and Horizon LOCI images"
msgstr "Neutron과 Horizon 이미지들"

msgid ""
"Note if you need to make a DNS change, you will have to do uninstall (``helm "
"delete <chart>``) and install again."
msgstr ""
"DNS 변경이 필요한 경우, 제거 (``helm delete <chart>``) 하고 다시 설치해야 할 "
"것임을 참고하십시오."

msgid ""
"Note that this command will only enable you to auth successfully using the "
"``python-openstackclient`` CLI. To use legacy clients like the ``python-"
"novaclient`` from the CLI, reference the auth values in ``/etc/openstack/"
"clouds.yaml`` and run::"
msgstr ""
"이 명령은 ``python-openstackclient`` CLI를 사용하여 성공적으로 인증할 수 있"
"을 것임을 참고하십시오. CLI에서 ``python-novaclient`` 와 같은 레거시 클라이언"
"트를 사용하려면, ``/etc/openstack/clouds.yaml`` 의 인증값을 참조하고 다음을 "
"실행합니다:"

msgid ""
"On the host or master node, install the latest versions of Git, CA Certs & "
"Make if necessary"
msgstr ""
"필요한 경우, 호스트 또는 마스터 노드에 최신 버전의 Git, CA Certs, 그리고 Make"
"를 설치하십시오"

msgid "On the master node create an environment file for the cluster:"
msgstr "Master 노드에서 클러스터에 대한 환경 파일을 만듭니다:"

msgid "On the master node create an inventory file for the cluster:"
msgstr "클러스터에 대한 인벤토리 파일을 master 노드에 생성합니다:"

msgid "On the master node run the playbooks:"
msgstr "Master 노드에서 플레이북 실행:"

msgid "On the worker nodes:"
msgstr "Worker 노드에서:"

msgid ""
"Once OpenStack-Helm has been deployed, the cloud can be exercised either "
"with the OpenStack client, or the same heat templates that are used in the "
"validation gates."
msgstr ""
"OpenStack-Helm이 배포되면, OpenStack client 또는 검증 게이트에서 사용되는 것"
"과 동일한 heat 템플릿으로 클라우드를 실습할 수 있습니다."

msgid ""
"Once installed, access the API's or Dashboard at `http://horizon.os.foo.org`"
msgstr ""
"설치가 완료되면, API's 나 `http://horizon.os.foo.org` 대시보드로 접속합니다."

msgid ""
"Once the host has been configured the repos containing the OpenStack-Helm "
"charts should be cloned onto each node in the cluster:"
msgstr ""
"호스트가 구성되면, OpenStack-Helm chart 가 포함된 repo가 클러스터 각 노드에 "
"복제됩니다."

msgid ""
"Once the host has been configured the repos containing the OpenStack-Helm "
"charts should be cloned:"
msgstr ""
"호스트가 구성되면 OpenStack-Helm 차트가 포함된 저장소가 clone되어야 합니다:"

msgid ""
"OpenStack-Helm uses the hosts networking namespace for many pods including, "
"Ceph, Neutron and Nova components. For this, to function, as expected pods "
"need to be able to resolve DNS requests correctly. Ubuntu Desktop and some "
"other distributions make use of ``mdns4_minimal`` which does not operate as "
"Kubernetes expects with its default TLD of ``.local``. To operate at "
"expected either change the ``hosts`` line in the ``/etc/nsswitch.conf``, or "
"confirm that it matches:"
msgstr ""
"OpenStack-Helm은 Ceph, Neutron 및 Nova 구성 요소를 포함한 많은 pod를 위한 호"
"소트 네트워킹 네임스페이스를 사용합니다. 이를 위해, 기능적으로, 예상되는 pod"
"들은 DNS 요청을 정확하게 해결할 수 있을 필요가 있습니다. Ubuntu Desktop과 다"
"른 배포판은 Kubernetes가 ``.local`` 의 기본 TLD로 예상하여 동작하지 않는 "
"``mdns4_minimal`` 을 활용합니다. 예상대로 동작하려면 ``/etc/nsswitch.conf`` "
"의 ``hosts`` 행을 변경하거나 일치하는지 확인하십시오:"

msgid ""
"OpenStack-Helm utilizes the `Kubernetes Ingress Controller <https://"
"kubernetes.io/docs/concepts/services-networking/ingress/>`__"
msgstr ""
"OpenStack-Helm 은 `Kubernetes Ingress Controller <https://kubernetes.io/docs/"
"concepts/services-networking/ingress/>`__ 을 활용합니다."

msgid "OpenStack-Helm-Infra KubeADM deployment"
msgstr "OpenStack-Helm-Infra KubeADM 배포"

msgid ""
"Other versions and considerations (such as other CNI SDN providers), config "
"map data, and value overrides will be included in other documentation as we "
"explore these options further."
msgstr ""
"다른 버전과 고려 사항들(다른 CNI SDN 제공자들과 같은), 설정 맵 데이터, 값 오"
"버라이드는 우리가 옵션들을 더 자세히 조사할때 다른 문서에 포함될 것입니다."

msgid "Overview"
msgstr "개요"

msgid "Passwordless Sudo"
msgstr "Sudo 비밀번호 생략"

msgid ""
"Please see the supported application versions outlined in the `source "
"variable file <https://github.com/openstack/openstack-helm-infra/blob/master/"
"roles/build-images/defaults/main.yml>`_."
msgstr ""
"`소스 변수 파일 <https://github.com/openstack/openstack-helm-infra/blob/"
"master/roles/build-images/defaults/main.yml>`_ 에 작성된 애플리케이션 버전을 "
"확인하십시오."

msgid "Prepare LOCI images"
msgstr "LOCI 이미지 준비"

msgid ""
"Prepare Neutron or Horizon LOCI image using this requirements image as :code:"
"`docker build --build-arg WHEELS` command argument."
msgstr ""
"Requirements 이미지를 :code:`docker build --build-arg WHEELS` 명령을 사용해"
"서 Neutron / Horizon LOCI 이미지를 준비합니다."

msgid ""
"Prepare a requirements LOCI image with Neutron TaaS and TaaS Dashboard code "
"installed."
msgstr ""
"Neutron TaaS, TaaS Dashboard 코드가 설치된 requirements LOCI 이미지를 준비합"
"니다."

msgid ""
"Prepare ahead of time your FQDN and DNS layouts. There are a handful of "
"OpenStack endpoints you will want to expose for API and Dashboard access."
msgstr ""
"미리 FQDN과 DNS 레이아웃을 준비하십시오. API와 대시보드 접근을 위해 공개하는 "
"소수의 OpenStack endpoint 가 있습니다."

msgid "Proxy Configuration"
msgstr "프록시 구성"

msgid "Removing Helm Charts"
msgstr "Helm 차트 제거"

msgid "Requirements"
msgstr "요구 사항"

msgid "Requirements LOCI image"
msgstr "Requirements LOCI 이미지"

msgid "Requirements and Host Configuration"
msgstr "요구 사항 및 호스트 구성"

msgid "Run the playbooks"
msgstr "플래이북 실행"

msgid "SSH-Key preparation"
msgstr "SSH-Key 준비"

msgid ""
"Set correct ownership: ``sudo chown ubuntu /etc/openstack-helm/deploy-key."
"pem``"
msgstr ""
"Ownership 변경: ``sudo chown ubuntu /etc/openstack-helm/deploy-key.pem``"

msgid "Setup Clients on the host and assemble the charts"
msgstr "호스트의 클라이언트 설치 및 차트 assemble"

msgid "Setup the gateway to the public network"
msgstr "공용 네트워크로의 게이트웨이 설치"

msgid "System Requirements"
msgstr "시스템 요구 사항"

msgid "TaaS Architecture"
msgstr "TaaS 아키텍처"

msgid ""
"TaaS plugin provides a mechanism to mirror certain traffic (for example "
"tagged with specific VLANs) from a source VM to any traffic analyzer VM. "
"When packet will be forwarded, the original value of source and target ip/"
"ports information will not be altered and the system administrator will be "
"able to run, for ex. tcpdump, on the target VM to trace these packets."
msgstr ""
"TaaS 플러그인은 소스 VM에서 모든 트레픽을 분석하는 VM으로 특정 트래픽 (예, 특"
"정 VLAN 테그)을 미러링하는 메커니즘을 제공합니다. 패킷이 전달 될때 원본과 타"
"겟 IP/Port 정보를 기존 값 변경없이 시스템 관리자가 실행할 수 있습니다. 예: 패"
"킷 추적을 위해 타겟 VM에서 tcpdump를 실행합니다"

msgid ""
"Test this by ssh'ing to a node and then executing a command with 'sudo'. "
"Neither operation should require a password."
msgstr ""
"노드에 ssh 로 접속하여 'sudo' 명령을 실행하여 테스트합니다. 이 작업에서는 암"
"호를 물어보지 않습니다."

msgid ""
"The OpenStack clients and Kubernetes RBAC rules, along with assembly of the "
"charts can be performed by running the following commands:"
msgstr ""
"차트 assembly와 마찬가지로, OpenStack 클라이언트 및 Kubernetes RBAC 규칙들은 "
"다음 명령을 실행하여 수행될 수 있습니다."

msgid ""
"The `./tools/deployment/multinode/kube-node-subnet.sh` script requires "
"docker to run."
msgstr ""
"`./tools/deployment/multinode/kube-node-subnet.sh`스크립트는 docker에서 실행"
"되어야 합니다."

msgid ""
"The ``.svc.cluster.local`` address is required to allow the OpenStack client "
"to communicate without being routed through proxy servers. The IP address "
"``172.17.0.1`` is the advertised IP address for the Kubernetes API server. "
"Replace the addresses if your configuration does not match the one defined "
"above."
msgstr ""
"``.svc.cluster.local`` 주소는 프록시 서버를 통해 라우팅하지 않고 OpenStack 클"
"라이언트와 통신할 수 있도록 하는 것을 필요로 합니다. IP 주소 ``172.17.0.1`` "
"은 Kubernetes API 서버를 위해 알려진 주소입니다. 구성이 위에 정의된 것과 일치"
"하지 않으면 주소를 교체하십시오."

msgid "The default FQDN's for OpenStack-Helm are"
msgstr "OpenStack-Helm을 위한 기본 FQDN은 다음과 같습니다."

msgid ""
"The example above uses the default values used by ``openstack-helm-infra``."
msgstr ""
"위의 예제는 ``openstack-helm-infra`` 에서 사용되는 기본값들을 사용합니다."

msgid ""
"The following commands all assume that they are run from the ``/opt/"
"openstack-helm`` directory."
msgstr ""
"다음에 나오는 명령들이 ``/opt/openstack-helm`` 디렉토리에서 실행된다고 가정합"
"니다."

msgid ""
"The following commands all assume that they are run from the ``openstack-"
"helm`` directory and the repos have been cloned as above."
msgstr ""
"다음 명령은 ``openstack-helm`` 디렉터리에서 실행되고 저장소가 위와 같이 clone"
"되어 있음을 가정합니다."

msgid ""
"The installation procedures below, will take an administrator from a new "
"``kubeadm`` installation to OpenStack-Helm deployment."
msgstr ""
"아래의 설치 과정은 관리자가 새로운 ``kubeadm`` 설치에서 OpenStack-Helm 배포"
"가 가능하게 합니다."

msgid "The recommended minimum system requirements for a full deployment are:"
msgstr "전체 배포를 위해 권장되는 시스템 최소 요구 사항:"

msgid ""
"The upstream Ceph image repository does not currently pin tags to specific "
"Ceph point releases.  This can lead to unpredictable results in long-lived "
"deployments.  In production scenarios, we strongly recommend overriding the "
"Ceph images to use either custom built images or controlled, cached images."
msgstr ""
"업스트림 Ceph 이미지 저장소는 현재 특정 Ceph 포인트 릴리즈에 태그를 지정하지 "
"않습니다. 이로 인해 오래전 배포된 시스템에서는 예기치 않은 결과가 발생할 수 "
"있습니다. 프로덕션 시나리오에서는 Ceph 이미지를 재정의하여 커스텀 빌드 이미"
"지 또는 제어된 캐시 이미지를 사용하는 것이 좋습니다."

msgid ""
"These commands will restore the environment back to a clean Kubernetes "
"deployment, that can either be manually removed or over-written by "
"restarting the deployment process. It is recommended to restart the host "
"before doing so to ensure any residual state, eg. Network interfaces are "
"removed."
msgstr ""
"이 명령들은 배포 프로세스를 재시작하여 수동으로 제거하거나 덮어써서, "
"Kubernetes 배포 환경으로 복구할 것입니다. 잔여 상태를 확인하기 전에 호스트를 "
"다시 시작하는 것을 권장합니다. 예시. 네트워크 인터페이스 제거 여부."

msgid ""
"This command will deploy a single node KubeADM administered cluster. This "
"will use the parameters in ``${OSH_INFRA_PATH}/playbooks/vars.yaml`` to "
"control the deployment, which can be over-ridden by adding entries to ``"
"${OSH_INFRA_PATH}/tools/gate/devel/local-vars.yaml``."
msgstr ""
"이 명령은 단일 노드 KubeADM 관리 클러스터를 배포할 것입니다. 이것은 ``"
"${OSH_INFRA_PATH}/tools/gate/devel/local-vars.yaml`` 에 항목들을 추가하여 오"
"버라이드한 ``${OSH_INFRA_PATH}/playbooks/vars.yaml`` 에 있는 매개변수를 사용"
"하여 배포를 제어합니다."

msgid ""
"This guide assumes that users wishing to deploy behind a proxy have already "
"defined the conventional proxy environment variables ``http_proxy``, "
"``https_proxy``, and ``no_proxy``."
msgstr ""
"이 가이드는 프록시에서 배포하려는 사용자가 이미 일반 프록시 환경 변수 "
"``http_proxy``, ``https_proxy``, 그리고 ``no_proxy`` 를 정의했다고 가정합니"
"다."

msgid "This guide covers the minimum number of requirements to get started."
msgstr "이 가이드는 시작을 위한 최소한의 요구 사항들을 다룹니다."

msgid ""
"This guide explains how to deploy tap-as-a-service (TaaS) Neutron plugin and "
"TaaS Dashboard plugin in Neutron and Horizon charts respectively."
msgstr ""
"이 가이드는 Neutron과 Horizon 차트에서의 TaaS (Tap-as-a-Service) Neutron 플러"
"그인과 TaaS Dashboard 플러그인을 배포하는 방법에 대해서 설명합니다."

msgid ""
"This installation, by default will use Google DNS servers, 8.8.8.8 or "
"8.8.4.4 and updates resolv.conf. These DNS nameserver entries can be changed "
"by updating file ``/opt/openstack-helm-infra/tools/images/kubeadm-aio/assets/"
"opt/playbooks/vars.yaml`` under section ``external_dns_nameservers``. This "
"change must be done on each node in your cluster."
msgstr ""
"설치시 기본적으로 Google DNS 서버 (8.8.8.8, 8.8.4.4)를 사용하고 resolv.conf "
"파일을 업데이트합니다. 이러한 DNS 네임서버 항목은 ``/opt/openstack-helm/"
"infra/tools/images/kubeadm-aio/asserts/opt/playbooks/vars.yaml`` 파일에서 "
"``external_dns_nameservers`` 섹션에서 업데이트하여 변경합니다. 이 변경은 클러"
"스터에 속한 모든 노드에서 수행해야합니다."

msgid "This is a two step process, i.e."
msgstr "두 단계 프로세스 입니다."

msgid ""
"This will delete all Kubernetes resources generated when the chart was "
"instantiated. However for OpenStack charts, by default, this will not delete "
"the database and database users that were created when the chart was "
"installed. All OpenStack projects can be configured such that upon deletion, "
"their database will also be removed. To delete the database when the chart "
"is deleted the database drop job must be enabled before installing the "
"chart. There are two ways to enable the job, set the job_db_drop value to "
"true in the chart's ``values.yaml`` file, or override the value using the "
"helm install command as follows:"
msgstr ""
"이는 차트가 인스턴스화될 때 생성된 모든 Kubernetes 자원을 삭제할 것입니다. 그"
"러나 OpenStack 차트의 경우, 기본적으로 차트를 설치할 때 생성된 데이터베이스"
"와 데이터베이스 사용자는 삭제하지 않을 것입니다. 모든 OpenStack 프로젝트는 삭"
"제 중에, 해당 데이터베이스 또한 제거되도록 구성할 수 있습니다. 차트가 삭제될 "
"때 데이터베이스를 삭제하려면 차트를 설치하기 전에 데이터베이스 drop 작업이 사"
"용 가능해야합니다. 작업을 사용 가능하도록 하는 두 가지 방법이 있습니다, 차트"
"의 ``values.yaml`` 파일의 job_db_drop의 값을 true로 설정하거나, 다음의 helm "
"설치 명령을 사용하여 값을 오버라이드하십시오:"

msgid ""
"Throughout this guide the assumption is that the user is: ``ubuntu``. "
"Because this user has to execute root level commands remotely to other "
"nodes, it is advised to add the following lines to ``/etc/sudoers`` for each "
"node:"
msgstr ""
"이 가이드 전체에서 사용자: ``ubuntu`` 라 가정합니다. 이 사용자는 root 수준의 "
"명령을 다른 노드에 원격으로 실행해야하기 때문에, 각 노드의 ``/etc/sudoers`` "
"에 다음 줄을 추가하는 것을 권장합니다:"

msgid ""
"To copy the ssh key to each node, this can be accomplished with the ``ssh-"
"copy-id`` command, for example: *ssh-copy-id ubuntu@192.168.122.178*"
msgstr ""
"각 노드에 ssh 키를 복사하려면, ``ssh-copy-id`` 명령을 사용하면됩니다. 예를 들"
"어: *ssh-copy-id ubuntu@192.168.122.178*"

msgid "To delete an installed helm chart, use the following command:"
msgstr "설치된 helm 차트를 제거하려면, 다음의 명령을 사용하십시오:"

msgid "To generate the key you can use ``ssh-keygen -t rsa``"
msgstr "``ssh-keygen -t rsa`` 명령을 이용하여 키를 생성합니다."

msgid ""
"To run further commands from the CLI manually, execute the following to set "
"up authentication credentials::"
msgstr ""
"CLI 에서 수동으로 더 많은 명령들을 실행하기 위해, 다음의 인증 자격 증명서 설"
"정을 실행하십시오:"

msgid ""
"To tear-down, the development environment charts should be removed first "
"from the 'openstack' namespace and then the 'ceph' namespace using the "
"commands from the `Removing Helm Charts` section. Additionally charts should "
"be removed from the 'nfs' and 'libvirt' namespaces if deploying with NFS "
"backing or bare metal development support. You can run the following "
"commands to loop through and delete the charts, then stop the kubelet "
"systemd unit and remove all the containers before removing the directories "
"used on the host by pods."
msgstr ""
"해제하기 위해, 개발 환경 차트는 `Removing Helm Chart` 섹션의 명령을 사용하여 "
"먼저 \\'openstack\\' 네임스페이스에서 제거되고 \\'ceph\\' 네임스페이스에서 제"
"거되어야 합니다. NFS backing 이나 베어 메탈 개발 지원으로 배포하려면, 추가적"
"인 차트는 \\'nfs\\' 와 \\'libvirt\\' 네임스페이스에서 제거되어야 합니다. 루프"
"문을 통해 차트들을 삭제하고, kubelet systemd unit 을 중지하며, pod 별로 호스"
"트에서 사용되는 디렉터리를 삭제하기 전에 모든 컨테이너들을 삭제합니다."

msgid "Troubleshooting"
msgstr "트러블 슈팅"

msgid ""
"Two similar options exist to set the FQDN overrides for External DNS mapping."
msgstr ""
"외부 DNS 매핑을 위한 FQDN 오버라이드를 설정하는 두 가지 유사한 옵션이 존재합"
"니다."

msgid ""
"Until the Ubuntu kernel shipped with 16.04 supports CephFS subvolume mounts "
"by default the `HWE Kernel <../troubleshooting/ubuntu-hwe-kernel.html>`__ is "
"required to use CephFS."
msgstr ""
"Ubuntu 16.04 커널에서 서브 볼륨 마운트로 CephFS를 지원하기 전까지 `HWE "
"Kernel <../troubleshooting/ubuntu-hwe-kernel.html>`__ 을 이용하여 CephFS를 사"
"용해야합니다."

msgid ""
"Update your lab/environment DNS server with your appropriate host values "
"creating A Records for the edge node IP's and various FQDN's. Alternatively "
"you can test these settings locally by editing your ``/etc/hosts``. Below is "
"an example with a dummy domain ``os.foo.org`` and dummy Ingress IP "
"``1.2.3.4``."
msgstr ""
"엣지 노드 IP와 다양한 FQDN에 대한 A Record를 생성하여 적절한 호스트 값으로 실"
"험실/환경 DNS 서버를 갱신합니다. 대안으로 ``/etc/hosts`` 를 편집하여 설정을 "
"지역적으로 테스트할 수 있습니다. 아래는 더미 도메인 ``os.foo.org`` 와 더미 "
"Ingress IP ``1.2.3.4`` 로 된 예시입니다."

msgid "Using Horizon as an example, find the ``endpoints`` config."
msgstr "Horizon을 예로 들 때, ``endpoints`` 구성을 찾으십시오."

msgid ""
"Using the Helm packages previously pushed to the local Helm repository, run "
"the following commands to instruct tiller to create an instance of the given "
"chart. During installation, the helm client will print useful information "
"about resources created, the state of the Helm releases, and whether any "
"additional configuration steps are necessary."
msgstr ""
"이전에 로컬 Helm 저장소로 push된 Helm 패키지를 사용하여, tiller가 주어진 차트"
"의 인스턴스를 생성하는 것을 수행하도록 다음 명령을 실행하십시오. 설치 중에, "
"helm 클라이언트는 자원의 생성, Helm 릴리즈 상태, 그리고 추가 구성 단계가 필요"
"한지에 대한 유용한 정보를 출력할 것입니다."

msgid "Verify the *v1beta1/Ingress* resource has a Host with your FQDN value"
msgstr "*v1beta1/Ingress* 리소스에 FQDN 값이 있는 호스트가 있는지 확인합니다."

msgid ""
"We want to change the **public** configurations to match our DNS layouts "
"above. In each Chart ``values.yaml`` is a ``endpoints`` configuration that "
"has ``host_fqdn_override``'s for each API that the Chart either produces or "
"is dependent on. `Read more about how Endpoints are developed <https://docs."
"openstack.org/openstack-helm/latest/devref/endpoints.html>`__. Note while "
"Glance Registry is listening on a Ingress http endpoint, you will not need "
"to expose the registry for external services."
msgstr ""
"**public** 구성을 위의 DNS 레이아웃과 일치하도록 변경하고자 합니다. 각 차트에"
"서 ``values.yaml`` 은 차트를 생산하거나 종속하는 각 API에 대해 "
"``host_fqdn_override`` 를 갖고 있는 ``endpoints`` 구성입니다. `어떻게 "
"Endpoint가 개발되는지에 대해 더 읽어보십시오 <https://docs.openstack.org/"
"openstack-helm/latest/devref/endpoints.html>`__ . Glance 레지스트리가 "
"Ingress http endpoint에서 수신 대기하는 동안, 외부 서비스를 위해 레지스트리"
"를 노출할 필요가 없을 것임을 참고하십시오."

msgid ""
"You can use any Kubernetes deployment tool to bring up a working Kubernetes "
"cluster for use with OpenStack-Helm. For production deployments, please "
"choose (and tune appropriately) a highly-resilient Kubernetes distribution, "
"e.g.:"
msgstr ""
"Kubernetes 배포 도구를 사용하여 OpenStack-Helm 과 함께 사용할 수 있는 "
"Kubernetes 클러스터를 불러올 수 있습니다. 프로덕션 배포의 경우, 탄력적인 "
"Kubernetes 배포를 선택하고 적절히 조정하십시오. 예:"

msgid ""
"You can use any Kubernetes deployment tool to bring up a working Kubernetes "
"cluster for use with OpenStack-Helm. This guide describes how to simply "
"stand up a multinode Kubernetes cluster via the OpenStack-Helm gate scripts, "
"which use KubeADM and Ansible. Although this cluster won't be production-"
"grade, it will serve as a quick starting point in a lab or proof-of-concept "
"environment."
msgstr ""
"Kubernetes 배포 도구를 사용하여 OpenStack-Helm과 함께 사용할 수 있는 "
"Kubernetes 클러스터를 불러올 수 있습니다. 이 가이드는 KubeADM과 Ansible을 사"
"용하는 OpenStack-Helm 게이트 스크립트를 통해 간단하게 멀티 노드 Kubernetes 클"
"러스터를 구성하는 방법에 대해 설명합니다. 이 클러스터는 프로덕션 수준은 아니"
"지만 lab 이나 PoC 환경에서 빠른 시작을 할 수 있는 역할을 합니다."

msgid ""
"You may now deploy kubernetes, and helm onto your machine, first move into "
"the ``openstack-helm`` directory and then run the following:"
msgstr ""
"이제 kubernetes와 helm을 머신에 배포할 수 있습니다. 먼저 ``openstack-helm`` "
"디렉터리로 이동하여 다음을 실행하십시오:"

msgid ""
"`Airship <https://airshipit.org/>`_, a declarative open cloud infrastructure "
"platform"
msgstr ""
"`Airship <https://airshipit.org/>`_, 선언형 오픈 클라우드 인프라 플랫폼"

msgid ""
"`KubeADM <https://kubernetes.io/docs/setup/independent/high-availability/"
">`_, the foundation of a number of Kubernetes installation solutions"
msgstr ""
"`KubeADM <https://kubernetes.io/docs/setup/independent/high-availability/"
">`_, 다수의 노드에 기초 구성이 가능한 Kubernetes 설치 솔루션"

msgid ""
"node_one, node_two and node_three below are all worker nodes, children of "
"the master node that the commands below are executed on."
msgstr ""
"아래의 node_one, node_two, node_three는 현재 클러스터의 worker 노드이며, 아"
"래 명령이 실행되는 master 노드의 하위 노드입니다."
